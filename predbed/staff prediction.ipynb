{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Day_Index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Is_Weekend",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Total_Admissions_Today",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Discharges_Today",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ICU_Patients_Today",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Surgeries_Today",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Beds_Occupied_Today",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Beds_Required_Tomorrow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Doctors_Required_Tomorrow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Nurses_Required_Tomorrow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Technicians_Required_Tomorrow",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d99c9c9c-25e8-4c29-8147-c71a6eb13da4",
       "rows": [
        [
         "0",
         "0",
         "False",
         "43",
         "24",
         "8",
         "1",
         "139",
         "136",
         "13",
         "31",
         "3"
        ],
        [
         "1",
         "1",
         "False",
         "37",
         "20",
         "9",
         "2",
         "156",
         "156",
         "13",
         "31",
         "5"
        ],
        [
         "2",
         "2",
         "False",
         "16",
         "33",
         "3",
         "3",
         "139",
         "138",
         "7",
         "17",
         "4"
        ],
        [
         "3",
         "3",
         "False",
         "47",
         "21",
         "10",
         "0",
         "165",
         "170",
         "14",
         "33",
         "3"
        ],
        [
         "4",
         "4",
         "False",
         "42",
         "25",
         "8",
         "3",
         "182",
         "185",
         "15",
         "35",
         "5"
        ],
        [
         "5",
         "5",
         "True",
         "12",
         "9",
         "9",
         "2",
         "185",
         "181",
         "8",
         "19",
         "5"
        ],
        [
         "6",
         "6",
         "True",
         "13",
         "18",
         "5",
         "1",
         "180",
         "179",
         "5",
         "13",
         "3"
        ],
        [
         "7",
         "7",
         "False",
         "42",
         "16",
         "4",
         "0",
         "206",
         "205",
         "10",
         "25",
         "3"
        ],
        [
         "8",
         "8",
         "False",
         "18",
         "11",
         "5",
         "4",
         "213",
         "223",
         "9",
         "22",
         "5"
        ],
        [
         "9",
         "9",
         "False",
         "32",
         "35",
         "5",
         "3",
         "210",
         "214",
         "11",
         "27",
         "4"
        ],
        [
         "10",
         "10",
         "False",
         "35",
         "25",
         "11",
         "0",
         "220",
         "227",
         "12",
         "28",
         "3"
        ],
        [
         "11",
         "11",
         "False",
         "43",
         "24",
         "8",
         "0",
         "239",
         "242",
         "12",
         "29",
         "3"
        ],
        [
         "12",
         "12",
         "True",
         "10",
         "16",
         "3",
         "0",
         "233",
         "237",
         "5",
         "10",
         "3"
        ],
        [
         "13",
         "13",
         "True",
         "12",
         "7",
         "7",
         "1",
         "238",
         "243",
         "6",
         "15",
         "3"
        ],
        [
         "14",
         "14",
         "False",
         "42",
         "16",
         "3",
         "0",
         "264",
         "267",
         "9",
         "24",
         "3"
        ],
        [
         "15",
         "15",
         "False",
         "37",
         "33",
         "9",
         "2",
         "268",
         "267",
         "13",
         "31",
         "5"
        ],
        [
         "16",
         "16",
         "False",
         "15",
         "44",
         "4",
         "0",
         "239",
         "240",
         "5",
         "11",
         "3"
        ],
        [
         "17",
         "17",
         "False",
         "40",
         "23",
         "11",
         "1",
         "256",
         "264",
         "14",
         "33",
         "4"
        ],
        [
         "18",
         "18",
         "False",
         "27",
         "41",
         "5",
         "4",
         "242",
         "246",
         "11",
         "26",
         "5"
        ],
        [
         "19",
         "19",
         "True",
         "16",
         "17",
         "4",
         "0",
         "241",
         "245",
         "5",
         "12",
         "3"
        ],
        [
         "20",
         "20",
         "True",
         "28",
         "6",
         "4",
         "1",
         "263",
         "274",
         "8",
         "20",
         "3"
        ],
        [
         "21",
         "21",
         "False",
         "25",
         "37",
         "2",
         "0",
         "251",
         "259",
         "6",
         "14",
         "3"
        ],
        [
         "22",
         "22",
         "False",
         "27",
         "12",
         "12",
         "0",
         "266",
         "277",
         "11",
         "25",
         "4"
        ],
        [
         "23",
         "23",
         "False",
         "47",
         "33",
         "9",
         "0",
         "280",
         "281",
         "13",
         "32",
         "3"
        ],
        [
         "24",
         "24",
         "False",
         "26",
         "11",
         "2",
         "0",
         "295",
         "304",
         "6",
         "15",
         "3"
        ],
        [
         "25",
         "25",
         "False",
         "16",
         "11",
         "6",
         "0",
         "300",
         "299",
         "6",
         "14",
         "3"
        ],
        [
         "26",
         "26",
         "True",
         "28",
         "6",
         "6",
         "2",
         "322",
         "324",
         "10",
         "24",
         "4"
        ],
        [
         "27",
         "27",
         "True",
         "32",
         "15",
         "6",
         "0",
         "339",
         "341",
         "9",
         "22",
         "3"
        ],
        [
         "28",
         "28",
         "False",
         "48",
         "15",
         "2",
         "0",
         "372",
         "382",
         "10",
         "26",
         "3"
        ],
        [
         "29",
         "29",
         "False",
         "20",
         "25",
         "13",
         "2",
         "367",
         "369",
         "12",
         "27",
         "6"
        ],
        [
         "30",
         "30",
         "False",
         "33",
         "35",
         "5",
         "3",
         "365",
         "372",
         "11",
         "27",
         "4"
        ],
        [
         "31",
         "31",
         "False",
         "47",
         "27",
         "13",
         "0",
         "385",
         "393",
         "15",
         "36",
         "4"
        ],
        [
         "32",
         "32",
         "False",
         "37",
         "40",
         "8",
         "3",
         "382",
         "391",
         "14",
         "32",
         "5"
        ],
        [
         "33",
         "33",
         "True",
         "25",
         "24",
         "1",
         "1",
         "383",
         "394",
         "6",
         "15",
         "3"
        ],
        [
         "34",
         "34",
         "True",
         "14",
         "9",
         "9",
         "2",
         "388",
         "390",
         "8",
         "20",
         "5"
        ],
        [
         "35",
         "35",
         "False",
         "30",
         "12",
         "9",
         "3",
         "406",
         "416",
         "13",
         "30",
         "6"
        ],
        [
         "36",
         "36",
         "False",
         "22",
         "29",
         "4",
         "0",
         "399",
         "411",
         "6",
         "15",
         "3"
        ],
        [
         "37",
         "37",
         "False",
         "32",
         "27",
         "14",
         "0",
         "404",
         "407",
         "13",
         "30",
         "4"
        ],
        [
         "38",
         "38",
         "False",
         "47",
         "13",
         "5",
         "2",
         "438",
         "439",
         "13",
         "32",
         "3"
        ],
        [
         "39",
         "39",
         "False",
         "47",
         "21",
         "8",
         "4",
         "464",
         "467",
         "17",
         "39",
         "6"
        ],
        [
         "40",
         "40",
         "True",
         "31",
         "12",
         "5",
         "1",
         "483",
         "484",
         "9",
         "22",
         "3"
        ],
        [
         "41",
         "41",
         "True",
         "24",
         "5",
         "4",
         "2",
         "502",
         "503",
         "8",
         "20",
         "3"
        ],
        [
         "42",
         "42",
         "False",
         "31",
         "37",
         "4",
         "3",
         "496",
         "505",
         "11",
         "25",
         "4"
        ],
        [
         "43",
         "43",
         "False",
         "38",
         "38",
         "4",
         "3",
         "496",
         "497",
         "12",
         "29",
         "4"
        ],
        [
         "44",
         "44",
         "False",
         "35",
         "41",
         "3",
         "0",
         "490",
         "500",
         "8",
         "20",
         "3"
        ],
        [
         "45",
         "45",
         "False",
         "45",
         "12",
         "3",
         "1",
         "523",
         "538",
         "11",
         "27",
         "3"
        ],
        [
         "46",
         "46",
         "False",
         "41",
         "11",
         "2",
         "0",
         "553",
         "565",
         "9",
         "22",
         "3"
        ],
        [
         "47",
         "47",
         "True",
         "31",
         "20",
         "7",
         "1",
         "564",
         "568",
         "10",
         "24",
         "3"
        ],
        [
         "48",
         "48",
         "True",
         "27",
         "17",
         "9",
         "0",
         "574",
         "579",
         "9",
         "22",
         "3"
        ],
        [
         "49",
         "49",
         "False",
         "15",
         "17",
         "13",
         "2",
         "572",
         "580",
         "11",
         "24",
         "6"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 730
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day_Index</th>\n",
       "      <th>Is_Weekend</th>\n",
       "      <th>Total_Admissions_Today</th>\n",
       "      <th>Total_Discharges_Today</th>\n",
       "      <th>ICU_Patients_Today</th>\n",
       "      <th>Surgeries_Today</th>\n",
       "      <th>Total_Beds_Occupied_Today</th>\n",
       "      <th>Total_Beds_Required_Tomorrow</th>\n",
       "      <th>Doctors_Required_Tomorrow</th>\n",
       "      <th>Nurses_Required_Tomorrow</th>\n",
       "      <th>Technicians_Required_Tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>136</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>170</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>182</td>\n",
       "      <td>185</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>725</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4362</td>\n",
       "      <td>4359</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>726</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4373</td>\n",
       "      <td>4378</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>727</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4376</td>\n",
       "      <td>4379</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>728</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4395</td>\n",
       "      <td>4390</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>729</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4415</td>\n",
       "      <td>4417</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>730 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Day_Index  Is_Weekend  Total_Admissions_Today  Total_Discharges_Today  \\\n",
       "0            0       False                      43                      24   \n",
       "1            1       False                      37                      20   \n",
       "2            2       False                      16                      33   \n",
       "3            3       False                      47                      21   \n",
       "4            4       False                      42                      25   \n",
       "..         ...         ...                     ...                     ...   \n",
       "725        725       False                      33                      14   \n",
       "726        726        True                      31                      20   \n",
       "727        727        True                      21                      18   \n",
       "728        728       False                      33                      14   \n",
       "729        729       False                      39                      19   \n",
       "\n",
       "     ICU_Patients_Today  Surgeries_Today  Total_Beds_Occupied_Today  \\\n",
       "0                     8                1                        139   \n",
       "1                     9                2                        156   \n",
       "2                     3                3                        139   \n",
       "3                    10                0                        165   \n",
       "4                     8                3                        182   \n",
       "..                  ...              ...                        ...   \n",
       "725                   4                0                       4362   \n",
       "726                   3                0                       4373   \n",
       "727                   9                2                       4376   \n",
       "728                  10                3                       4395   \n",
       "729                   8                0                       4415   \n",
       "\n",
       "     Total_Beds_Required_Tomorrow  Doctors_Required_Tomorrow  \\\n",
       "0                             136                         13   \n",
       "1                             156                         13   \n",
       "2                             138                          7   \n",
       "3                             170                         14   \n",
       "4                             185                         15   \n",
       "..                            ...                        ...   \n",
       "725                          4359                          8   \n",
       "726                          4378                          7   \n",
       "727                          4379                         10   \n",
       "728                          4390                         14   \n",
       "729                          4417                         11   \n",
       "\n",
       "     Nurses_Required_Tomorrow  Technicians_Required_Tomorrow  \n",
       "0                          31                              3  \n",
       "1                          31                              5  \n",
       "2                          17                              4  \n",
       "3                          33                              3  \n",
       "4                          35                              5  \n",
       "..                        ...                            ...  \n",
       "725                        20                              3  \n",
       "726                        18                              3  \n",
       "727                        23                              5  \n",
       "728                        32                              6  \n",
       "729                        27                              3  \n",
       "\n",
       "[730 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HARSHDIP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.0973 - val_loss: 0.0672\n",
      "Epoch 2/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0661 - val_loss: 0.0641\n",
      "Epoch 3/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0621 - val_loss: 0.0651\n",
      "Epoch 4/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0602 - val_loss: 0.0621\n",
      "Epoch 5/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0587 - val_loss: 0.0581\n",
      "Epoch 6/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0505 - val_loss: 0.0577\n",
      "Epoch 7/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0548 - val_loss: 0.0572\n",
      "Epoch 8/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0532 - val_loss: 0.0700\n",
      "Epoch 9/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0532 - val_loss: 0.0560\n",
      "Epoch 10/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0525 - val_loss: 0.0571\n",
      "Epoch 11/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0495 - val_loss: 0.0596\n",
      "Epoch 12/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0499 - val_loss: 0.0641\n",
      "Epoch 13/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0507 - val_loss: 0.0570\n",
      "Epoch 14/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0510 - val_loss: 0.0573\n",
      "Epoch 15/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0554 - val_loss: 0.0669\n",
      "Epoch 16/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0467 - val_loss: 0.0626\n",
      "Epoch 17/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0468 - val_loss: 0.0588\n",
      "Epoch 18/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0481 - val_loss: 0.0601\n",
      "Epoch 19/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0533 - val_loss: 0.0591\n",
      "Epoch 20/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0451 - val_loss: 0.0600\n",
      "Epoch 21/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0487 - val_loss: 0.0645\n",
      "Epoch 22/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0508 - val_loss: 0.0675\n",
      "Epoch 23/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0496 - val_loss: 0.0678\n",
      "Epoch 24/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0482 - val_loss: 0.0716\n",
      "Epoch 25/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0485 - val_loss: 0.0621\n",
      "Epoch 26/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0453 - val_loss: 0.0715\n",
      "Epoch 27/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0437 - val_loss: 0.0708\n",
      "Epoch 28/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0481 - val_loss: 0.1580\n",
      "Epoch 29/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0459 - val_loss: 0.0642\n",
      "Epoch 30/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0454 - val_loss: 0.0831\n",
      "Epoch 31/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0480 - val_loss: 0.0769\n",
      "Epoch 32/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0456 - val_loss: 0.0615\n",
      "Epoch 33/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0441 - val_loss: 0.0757\n",
      "Epoch 34/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0434 - val_loss: 0.0809\n",
      "Epoch 35/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0452 - val_loss: 0.0964\n",
      "Epoch 36/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0443 - val_loss: 0.0721\n",
      "Epoch 37/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0418 - val_loss: 0.0541\n",
      "Epoch 38/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0454 - val_loss: 0.0686\n",
      "Epoch 39/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0415 - val_loss: 0.0872\n",
      "Epoch 40/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0399 - val_loss: 0.0937\n",
      "Epoch 41/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0431 - val_loss: 0.0572\n",
      "Epoch 42/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0459 - val_loss: 0.0609\n",
      "Epoch 43/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0453 - val_loss: 0.0604\n",
      "Epoch 44/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0448 - val_loss: 0.0606\n",
      "Epoch 45/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0443 - val_loss: 0.0941\n",
      "Epoch 46/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0440 - val_loss: 0.0560\n",
      "Epoch 47/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0426 - val_loss: 0.0671\n",
      "Epoch 48/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0428 - val_loss: 0.0776\n",
      "Epoch 49/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0415 - val_loss: 0.0690\n",
      "Epoch 50/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0417 - val_loss: 0.0625\n",
      "Epoch 51/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0421 - val_loss: 0.0757\n",
      "Epoch 52/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0481 - val_loss: 0.0905\n",
      "Epoch 53/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0431 - val_loss: 0.0798\n",
      "Epoch 54/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0439 - val_loss: 0.0714\n",
      "Epoch 55/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0434 - val_loss: 0.0740\n",
      "Epoch 56/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0426 - val_loss: 0.1032\n",
      "Epoch 57/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0437 - val_loss: 0.0635\n",
      "Epoch 58/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0436 - val_loss: 0.0761\n",
      "Epoch 59/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0422 - val_loss: 0.0748\n",
      "Epoch 60/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0408 - val_loss: 0.0760\n",
      "Epoch 61/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0437 - val_loss: 0.0914\n",
      "Epoch 62/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0421 - val_loss: 0.0628\n",
      "Epoch 63/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0450 - val_loss: 0.0861\n",
      "Epoch 64/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0410 - val_loss: 0.0881\n",
      "Epoch 65/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0435 - val_loss: 0.0634\n",
      "Epoch 66/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0403 - val_loss: 0.0687\n",
      "Epoch 67/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0405 - val_loss: 0.0948\n",
      "Epoch 68/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0436 - val_loss: 0.0716\n",
      "Epoch 69/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0420 - val_loss: 0.0593\n",
      "Epoch 70/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0389 - val_loss: 0.0693\n",
      "Epoch 71/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0409 - val_loss: 0.0696\n",
      "Epoch 72/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0402 - val_loss: 0.0641\n",
      "Epoch 73/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0360 - val_loss: 0.0703\n",
      "Epoch 74/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0423 - val_loss: 0.0748\n",
      "Epoch 75/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0403 - val_loss: 0.0712\n",
      "Epoch 76/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0374 - val_loss: 0.0676\n",
      "Epoch 77/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0414 - val_loss: 0.0714\n",
      "Epoch 78/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0401 - val_loss: 0.0600\n",
      "Epoch 79/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0334 - val_loss: 0.0646\n",
      "Epoch 80/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0419 - val_loss: 0.0632\n",
      "Epoch 81/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0408 - val_loss: 0.0653\n",
      "Epoch 82/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0405 - val_loss: 0.0843\n",
      "Epoch 83/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0396 - val_loss: 0.0682\n",
      "Epoch 84/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0385 - val_loss: 0.0776\n",
      "Epoch 85/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0371 - val_loss: 0.0703\n",
      "Epoch 86/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0389 - val_loss: 0.0766\n",
      "Epoch 87/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0355 - val_loss: 0.0698\n",
      "Epoch 88/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0369 - val_loss: 0.0712\n",
      "Epoch 89/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0392 - val_loss: 0.0622\n",
      "Epoch 90/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0385 - val_loss: 0.0711\n",
      "Epoch 91/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0369 - val_loss: 0.0750\n",
      "Epoch 92/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0377 - val_loss: 0.0743\n",
      "Epoch 93/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0374 - val_loss: 0.0606\n",
      "Epoch 94/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0377 - val_loss: 0.0602\n",
      "Epoch 95/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0375 - val_loss: 0.0665\n",
      "Epoch 96/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0341 - val_loss: 0.0651\n",
      "Epoch 97/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0355 - val_loss: 0.0642\n",
      "Epoch 98/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0360 - val_loss: 0.0806\n",
      "Epoch 99/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0339 - val_loss: 0.0762\n",
      "Epoch 100/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0332 - val_loss: 0.0758\n",
      "Epoch 101/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0413 - val_loss: 0.0701\n",
      "Epoch 102/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0346 - val_loss: 0.0695\n",
      "Epoch 103/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0376 - val_loss: 0.0645\n",
      "Epoch 104/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0330 - val_loss: 0.0681\n",
      "Epoch 105/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0365 - val_loss: 0.0760\n",
      "Epoch 106/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0358 - val_loss: 0.0717\n",
      "Epoch 107/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0334 - val_loss: 0.0699\n",
      "Epoch 108/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0343 - val_loss: 0.0675\n",
      "Epoch 109/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0325 - val_loss: 0.0775\n",
      "Epoch 110/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0323 - val_loss: 0.0717\n",
      "Epoch 111/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0302 - val_loss: 0.0744\n",
      "Epoch 112/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0312 - val_loss: 0.0872\n",
      "Epoch 113/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0381 - val_loss: 0.0706\n",
      "Epoch 114/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0302 - val_loss: 0.0787\n",
      "Epoch 115/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0297 - val_loss: 0.0764\n",
      "Epoch 116/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0347 - val_loss: 0.0783\n",
      "Epoch 117/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0304 - val_loss: 0.0747\n",
      "Epoch 118/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0286 - val_loss: 0.0766\n",
      "Epoch 119/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0295 - val_loss: 0.0813\n",
      "Epoch 120/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0303 - val_loss: 0.0769\n",
      "Epoch 121/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0281 - val_loss: 0.0754\n",
      "Epoch 122/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0264 - val_loss: 0.0762\n",
      "Epoch 123/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0270 - val_loss: 0.0757\n",
      "Epoch 124/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0291 - val_loss: 0.0827\n",
      "Epoch 125/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0263 - val_loss: 0.0857\n",
      "Epoch 126/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0245 - val_loss: 0.0841\n",
      "Epoch 127/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0228 - val_loss: 0.0823\n",
      "Epoch 128/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0248 - val_loss: 0.0960\n",
      "Epoch 129/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0244 - val_loss: 0.0774\n",
      "Epoch 130/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0229 - val_loss: 0.0826\n",
      "Epoch 131/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0195 - val_loss: 0.0785\n",
      "Epoch 132/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0215 - val_loss: 0.0846\n",
      "Epoch 133/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0205 - val_loss: 0.0803\n",
      "Epoch 134/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0196 - val_loss: 0.0732\n",
      "Epoch 135/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0224 - val_loss: 0.0816\n",
      "Epoch 136/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0190 - val_loss: 0.0839\n",
      "Epoch 137/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0183 - val_loss: 0.0875\n",
      "Epoch 138/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0171 - val_loss: 0.0940\n",
      "Epoch 139/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0183 - val_loss: 0.0881\n",
      "Epoch 140/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0183 - val_loss: 0.0830\n",
      "Epoch 141/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0162 - val_loss: 0.0834\n",
      "Epoch 142/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0166 - val_loss: 0.0802\n",
      "Epoch 143/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0148 - val_loss: 0.0875\n",
      "Epoch 144/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0128 - val_loss: 0.0827\n",
      "Epoch 145/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0122 - val_loss: 0.0866\n",
      "Epoch 146/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0133 - val_loss: 0.0854\n",
      "Epoch 147/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0135 - val_loss: 0.0898\n",
      "Epoch 148/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0123 - val_loss: 0.0807\n",
      "Epoch 149/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0131 - val_loss: 0.0940\n",
      "Epoch 150/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0114 - val_loss: 0.0867\n",
      "Epoch 151/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0111 - val_loss: 0.0897\n",
      "Epoch 152/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0114 - val_loss: 0.0891\n",
      "Epoch 153/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0115 - val_loss: 0.0903\n",
      "Epoch 154/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0107 - val_loss: 0.0881\n",
      "Epoch 155/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0915\n",
      "Epoch 156/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0869\n",
      "Epoch 157/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0106 - val_loss: 0.0890\n",
      "Epoch 158/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0084 - val_loss: 0.0831\n",
      "Epoch 159/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0088 - val_loss: 0.0864\n",
      "Epoch 160/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0095 - val_loss: 0.0867\n",
      "Epoch 161/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0089 - val_loss: 0.0844\n",
      "Epoch 162/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0087 - val_loss: 0.0890\n",
      "Epoch 163/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0083 - val_loss: 0.0919\n",
      "Epoch 164/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0086 - val_loss: 0.0855\n",
      "Epoch 165/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0907\n",
      "Epoch 166/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0083 - val_loss: 0.0855\n",
      "Epoch 167/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0081 - val_loss: 0.0884\n",
      "Epoch 168/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0846\n",
      "Epoch 169/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0066 - val_loss: 0.0884\n",
      "Epoch 170/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0066 - val_loss: 0.0858\n",
      "Epoch 171/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0868\n",
      "Epoch 172/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0074 - val_loss: 0.0855\n",
      "Epoch 173/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0055 - val_loss: 0.0913\n",
      "Epoch 174/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0866\n",
      "Epoch 175/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0894\n",
      "Epoch 176/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0830\n",
      "Epoch 177/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0070 - val_loss: 0.0835\n",
      "Epoch 178/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0059 - val_loss: 0.0883\n",
      "Epoch 179/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0056 - val_loss: 0.0866\n",
      "Epoch 180/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0063 - val_loss: 0.0849\n",
      "Epoch 181/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0057 - val_loss: 0.0882\n",
      "Epoch 182/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0056 - val_loss: 0.0887\n",
      "Epoch 183/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0052 - val_loss: 0.0869\n",
      "Epoch 184/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0876\n",
      "Epoch 185/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0053 - val_loss: 0.0870\n",
      "Epoch 186/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0876\n",
      "Epoch 187/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0897\n",
      "Epoch 188/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0049 - val_loss: 0.0874\n",
      "Epoch 189/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0869\n",
      "Epoch 190/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0864\n",
      "Epoch 191/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0864\n",
      "Epoch 192/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.0870\n",
      "Epoch 193/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0874\n",
      "Epoch 194/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0885\n",
      "Epoch 195/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0044 - val_loss: 0.0846\n",
      "Epoch 196/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0877\n",
      "Epoch 197/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0843\n",
      "Epoch 198/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0846\n",
      "Epoch 199/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0895\n",
      "Epoch 200/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0855\n",
      "Trained and saved new model.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n",
      "Predicted Doctors Required for Tomorrow: 10.77\n",
      "Predicted Nurses Required for Tomorrow: 26.73\n",
      "Predicted Technicians Required for Tomorrow: 2.93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"staff_data.csv\")\n",
    "\n",
    "# Convert Date column to datetime and create Is_Weekend feature\n",
    "if \"Date\" in df.columns:\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"Is_Weekend\"] = df[\"Date\"].dt.dayofweek.isin([5, 6]).astype(int)  # 1 for Sat/Sun, 0 otherwise\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Define features for prediction\n",
    "features = [\n",
    "    \"Total_Admissions_Today\",\n",
    "    \"Total_Discharges_Today\",\n",
    "    \"ICU_Patients_Today\",\n",
    "    \"Surgeries_Today\",\n",
    "    \"Total_Beds_Occupied_Today\",\n",
    "    \"Total_Beds_Required_Tomorrow\",  # Using predicted bed requirement\n",
    "    \"Is_Weekend\"\n",
    "]\n",
    "\n",
    "# Targets (Staff predictions)\n",
    "targets = [\"Doctors_Required_Tomorrow\", \"Nurses_Required_Tomorrow\", \"Technicians_Required_Tomorrow\"]\n",
    "\n",
    "# Normalize numerical features (excluding Is_Weekend)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[features + targets] = scaler.fit_transform(df[features + targets])  # Scale all numerical data\n",
    "df_scaled = df_scaled.astype(np.float32)\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "train_size = int(len(df_scaled) * 0.8)\n",
    "train, test = df_scaled.iloc[:train_size], df_scaled.iloc[train_size:]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, seq_length=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i+seq_length][features].values)  # Features\n",
    "        y.append(data.iloc[i+seq_length][targets].values)  # Targets (staff predictions)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare sequences\n",
    "seq_length = 10  # Model looks at last 10 days\n",
    "X_train, y_train = create_sequences(train, seq_length)\n",
    "X_test, y_test = create_sequences(test, seq_length)\n",
    "\n",
    "# Define LSTM Model\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(seq_length, len(features))),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128),\n",
    "        Dropout(0.1),\n",
    "        Dense(len(targets))  # Output layer predicts all 3 staff categories\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Path for saving model\n",
    "model_path = \"lstm_staff_prediction.pkl\"\n",
    "\n",
    "# Load or Train Model\n",
    "if os.path.exists(model_path):\n",
    "#     pass\n",
    "#     # with open(model_path, \"rb\") as f:\n",
    "#     #     model_json, weights = pickle.load(f)\n",
    "#     # model = model_from_json(model_json)\n",
    "#     # model.set_weights(weights)\n",
    "#     # model.compile(loss='mse', optimizer='adam')\n",
    "#     # print(\"Loaded existing model.\")\n",
    "# else:\n",
    "    model = build_lstm_model()\n",
    "    model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Save model\n",
    "    model_json = model.to_json()\n",
    "    weights = model.get_weights()\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump((model_json, weights), f)\n",
    "\n",
    "    print(\"Trained and saved new model.\")\n",
    "\n",
    "# Predict next day's staff requirement\n",
    "latest_data = df_scaled.iloc[-seq_length:][features].values.reshape(1, seq_length, len(features))\n",
    "predicted_scaled = model.predict(latest_data)\n",
    "\n",
    "# Reverse scale the prediction\n",
    "predicted_full = np.zeros((1, len(features) + len(targets)))\n",
    "predicted_full[:, -len(targets):] = predicted_scaled.flatten()  # Insert predictions in target positions\n",
    "predicted_staff = scaler.inverse_transform(predicted_full)[:, -len(targets):][0]  # Extract only target values\n",
    "\n",
    "# Display predicted staff numbers\n",
    "print(f\"Predicted Doctors Required for Tomorrow: {predicted_staff[0]:.2f}\")\n",
    "print(f\"Predicted Nurses Required for Tomorrow: {predicted_staff[1]:.2f}\")\n",
    "print(f\"Predicted Technicians Required for Tomorrow: {predicted_staff[2]:.2f}\")\n",
    "\n",
    "# Function to update model with actual values\n",
    "# def update_model_with_actual(actual_staff):\n",
    "#     \"\"\"\n",
    "#     Updates the model with new actual data at the end of the day.\n",
    "#     \"\"\"\n",
    "#     global df_scaled, X_train, y_train\n",
    "\n",
    "#     # Add new actual data\n",
    "#     new_data = df.iloc[-1:].copy()\n",
    "#     new_data[targets] = actual_staff  # Use actual entered values\n",
    "\n",
    "#     # Normalize new data\n",
    "#     new_data_scaled = new_data.copy()\n",
    "#     new_data_scaled[features + targets] = scaler.transform(new_data[features + targets])\n",
    "\n",
    "#     # Append new data\n",
    "#     df_scaled = pd.concat([df_scaled, new_data_scaled])\n",
    "\n",
    "#     # Re-create sequences\n",
    "#     X_train, y_train = create_sequences(df_scaled, seq_length)\n",
    "\n",
    "#     # Fine-tune model with the new data\n",
    "#     model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "#     # Save updated model\n",
    "#     model_json = model.to_json()\n",
    "#     weights = model.get_weights()\n",
    "#     with open(model_path, \"wb\") as f:\n",
    "#         pickle.dump((model_json, weights), f)\n",
    "\n",
    "#     print(\"Model updated with new data.\")\n",
    "\n",
    "# # Get actual values entered by hospital staff\n",
    "# actual_doctors = float(input(\"Enter actual doctors required for today: \"))\n",
    "# actual_nurses = float(input(\"Enter actual nurses required for today: \"))\n",
    "# actual_technicians = float(input(\"Enter actual technicians required for today: \"))\n",
    "\n",
    "# # Compare and update model\n",
    "# print(f\"Predicted Doctors: {predicted_staff[0]:.2f}, Actual: {actual_doctors:.2f}\")\n",
    "# print(f\"Predicted Nurses: {predicted_staff[1]:.2f}, Actual: {actual_nurses:.2f}\")\n",
    "# print(f\"Predicted Technicians: {predicted_staff[2]:.2f}, Actual: {actual_technicians:.2f}\")\n",
    "\n",
    "# update_model_with_actual([actual_doctors, actual_nurses, actual_technicians])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "R² Score for Doctors Prediction: -0.3343\n",
      "R² Score for Nurses Prediction: -0.3904\n",
      "R² Score for Technicians Prediction: -0.3615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Reverse scale the predictions\n",
    "y_pred_full = np.zeros((len(y_pred_scaled), len(features) + len(targets)))\n",
    "y_pred_full[:, -len(targets):] = y_pred_scaled  # Place predictions in target positions\n",
    "y_pred = scaler.inverse_transform(y_pred_full)[:, -len(targets):]  # Extract only target values\n",
    "\n",
    "# Reverse scale actual test values\n",
    "y_test_full = np.zeros((len(y_test), len(features) + len(targets)))\n",
    "y_test_full[:, -len(targets):] = y_test  # Place actual values in target positions\n",
    "y_actual = scaler.inverse_transform(y_test_full)[:, -len(targets):]  # Extract only target values\n",
    "\n",
    "# Compute R² score for each staff type\n",
    "r2_doctors = r2_score(y_actual[:, 0], y_pred[:, 0])\n",
    "r2_nurses = r2_score(y_actual[:, 1], y_pred[:, 1])\n",
    "r2_technicians = r2_score(y_actual[:, 2], y_pred[:, 2])\n",
    "\n",
    "# Display R² scores\n",
    "print(f\"R² Score for Doctors Prediction: {r2_doctors:.4f}\")\n",
    "print(f\"R² Score for Nurses Prediction: {r2_nurses:.4f}\")\n",
    "print(f\"R² Score for Technicians Prediction: {r2_technicians:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
